# is hit or removes any steps outsite the boundary
if (stop){
# If one of the random steps leaves the extent, we break the loop
extent  <- extent(y) %>% as(., "SpatialPolygons")
inside  <- gContainsProperly(extent, steps, byid = TRUE) %>% as.vector()
if (sum(!inside) > 0) break
} else {
# Remove any point/step that does not fully lie withing the boundaries of
# our map (otherwise we can't extract the covariates below)
extent  <- extent(y) %>% as(., "SpatialPolygons")
keep    <- gContainsProperly(extent, steps, byid = TRUE) %>% as.vector()
steps   <- steps[keep, ]
rand    <- rand[keep, ]
}
# Add the data about turning angles etc. to the lines
steps <- SpatialLinesDataFrame(steps, data = rand, match.ID = F)
# Extract covariates along each random step
extracted <- extrCov(y, steps)
# Put some nice column names
names(extracted) <- names(y)
# Add extracted data to the random points
rand <- cbind(rand, extracted)
# Calculate cos_ta and log_sl
rand$cos_ta <- cos(rand$ta_)
rand$log_sl <- log(rand$sl_)
# Calculate a selection score for each point/step
rand$SelectionScore <- sapply(1:length(z), function(x){
rand[, names(z[x])] * z[x]
}) %>% as.data.frame() %>% rowSums() %>% exp()
# Keep only the step with the highest score
rand <- subset(rand, SelectionScore == max(SelectionScore))
# In case several steps get the same score, choose one randomly
rand <- rand[sample(nrow(rand), 1), ]
# Make the selected location spatial
coordinates(rand) <- c("x", "y")
# Add the step to our track
track <- rbind(
track[, c("absta_", "ta_", "sl_")]
, rand[, c("absta_", "ta_", "sl_")]
)
}
return(track)
}
shiny::runApp('ownCloud/Dokumente/Bibliothek/Wissen/R-Scripts/Shiny/02_MovementSimulation')
library(fpc)
install.packages("fpc")
library(fpc)
round(bhattacharyya.dist(c(1,1),c(2,5),diag(2),diag(2)),digits=2)
help("bhattacharyya.dist")
help("bhattacharyya.dist")
diag(2)
library(NLMR)
library(fpc)
# Simulate random maps
r1 <- nlm_gaussianfield(ncol = 10, nrow = 10)
plot(r1)
library(raster)
plot(r1)
# Simulate random maps
r1 <- nlm_distancegradient(ncol = 10, nrow = 10, origin = c(5, 5, 5, 5))
# Simulate random maps
r1 <- nlm_distancegradient(ncol = 10, nrow = 10, origin = c(5, 5, 5, 5))
r1a <- r1 + 0.5 * nlm_gaussianfield(ncol = 10, nrow = 10)
r1b <- r1 + 1.0 * nlm_gaussianfield(ncol = 10, nrow = 10)
par(mfrow = c(2, 2))
plot(r1)
plot(r1a)
plot(r1b)
# Coerce maps to dataframes
r1 <- as.data.frame(r1, xy = T)
r1
r1a <- as.data.frame(r1a, xy = T)
r1b <- as.data.frame(r1b, xy = T)
library(raster)
library(NLMR)
library(fpc)
# Simulate random maps
r1 <- nlm_distancegradient(ncol = 10, nrow = 10, origin = c(5, 5, 5, 5))
r1a <- r1 + 0.5 * nlm_gaussianfield(ncol = 10, nrow = 10)
r1b <- r1 + 1.0 * nlm_gaussianfield(ncol = 10, nrow = 10)
# Visualize them
par(mfrow = c(2, 2))
plot(r1)
plot(r1a)
plot(r1b)
r <- r1
r <- as.data.frame(r, xy = T)
r
# Function to extract means and sds of each dimension
extractVals <- function(r){
r <- as.data.frame(r, xy = T)
results <- data.frame(
MeanX = mean(r$x)
, MeanY = mean(r$y)
, SDX = sd(r$x)
, SDY = sd(r$y)
)
return(results)
}
extractVals(r1)
cov(r$x, r$y)
list(
Means = c(mean(r$x), mean(r$y))
, Covar = cov(r$x, r$x)
)
cov(r$x, r$x)
r$x
r$x
cov(r$x, r$x)
bhattacharyya.dist(c(1,1),c(2,5),diag(2),diag(2)),digits=2
bhattacharyya.dist(c(1,1),c(2,5),diag(2),diag(2))
c(1,1)
diag(2)
list(
Means = c(mean(r$x), mean(r$y))
, Covar = cov(r$x, r$y)
)
cor(r$x, r$y)
list(
Means = c(mean(r$x), mean(r$y))
r <- r1
, Covar = cor(r$x, r$y)
)
r <- r1
cor(r$x, r$y)
r <- as.data.frame(r, xy = T)
cor(r$x, r$y)
cov(r$x, r$y)
cov(cbind(r$x, r$y))
# Function to extract means and sds of each dimension
extractVals <- function(r){
r <- as.data.frame(r, xy = T)
results <- list(
Means = c(mean(r$x), mean(r$y))
, Covar = cov(cbind(r$x, r$y))
)
return(results)
}
extractVals(r1)
# Calculate B's index
results_r1 <- extract_vals(r1)
# Calculate B's index
results_r1 <- extractVals(r1)
results_r1a <- extractVals(r1a)
results_r1b <- extractVals(r1b)
bhattacharyya.dist(results_r1$Means, results_r1a$Means, results_r1$Covar, results_r1a$Covar)
bhattacharyya.dist(results_r1$Means, results_r1b$Means, results_r1$Covar, results_r1b$Covar)
# Calculate B's index
results_r1 <- extractVals(r1)
results_r1a <- extractVals(r1a)
results_r1b <- extractVals(r1b)
results_r1
results_r1a
results_r1b
par(mfrow = c(2, 2))
plot(r1)
plot(r1a)
plot(r1b)
results_r1
results_r1b
results_r1a
# Function to extract means and sds of each dimension
extractVals <- function(r){
r <- as.data.frame(r, xy = T)
results <- list(
Means = c(mean(r$x), mean(r$y))
, Covar = cov(cbind(r$x, r$y))
)
return(results)
}
extractVals(r1)
extractVals(r1a)
extractVals(r1b)
plot(r1)
plot(r2)
plot(r1a)
plot(r1b)
mean(as.data.frame(r1, xy = T))
as.data.frame(r1, xy = T)
mean(as.data.frame(r1, xy = T)$x)
mean(as.data.frame(r1, xy = T)$y)
mean(as.data.frame(r1a, xy = T)$y)
mean(as.data.frame(r1a, xy = T)$x)
r <- r1
r
r <- as.data.frame(r, xy = T)
r
library(tidyverse)
r %>% group_by(x) %>% summarize(MeanX = mean(x))
meanx <- r %>% group_by(x) %>% summarize(MeanX = mean(x)) %>% .[["MeanX"]]
meany <- r %>% group_by(y) %>% summarize(MeanX = mean(y)) %>% .[["MeanY"]]
# Function to extract means and sds of each dimension
extractVals <- function(r){
r <- as.data.frame(r, xy = T)
meanx <- r %>% group_by(x) %>% summarize(MeanX = mean(x)) %>% .[["MeanX"]]
meany <- r %>% group_by(y) %>% summarize(MeanX = mean(y)) %>% .[["MeanY"]]
results <- list(
Means = c(meanx, meany)
, Covar = cov(cbind(r$x, r$y))
)
return(results)
}
r <- r1
r <- as.data.frame(r, xy = T)
r
plot(layer ~ x, data = r)
install.packages("tmap")
install.packages("sf")
install.packages("tmap")
install.packages("Orcs")
library(Orcs)
library(moveNT)
locs <- puechabonsp$relocs
data(puechabonsp)
locs <- puechabonsp$relocs
xy <- coordinates(locs)
df <- as.data.frame(locs)
da <- as.character(df$Date)
da <- as.POSIXct(strptime(as.character(df$Date),"%y%m%d", tz="Europe/Paris"))
litr <- as.ltraj(xy, da, id = id)
id <- df$Name
da <- as.character(df$Date)
da <- as.POSIXct(strptime(as.character(df$Date),"%y%m%d", tz="Europe/Paris"))
litr <- as.ltraj(xy, da, id = id)
out1<-loop(litr)
mean_weight<-mosaic_network(out1, index=2, sc=T, fun=mean) #Perform mean weight (not-interpolated)
plot(mean_weight)
sessionInfo()
library(moveNT)
traj1<-sim_mov(type="OU", npatches=3, grph=T)
adj<-traj2adj(traj1, res=100)
# DD discrete-time SIR model
# Intro & Data
rm(list=ls(all=TRUE))
library(jagsUI)
library(tidyverse)
library(MASS)
library(gridExtra)
#Simulate
# no birth, no bg mortality
time = 50
N0 = c(10000,1,0)
Nt = matrix(NA,time,3,dimnames=list(NULL,c('S','I','R')))
Nt[1,] = N0
for(t in 2:time){
S=Nt[t-1,1]
I=Nt[t-1,2]
R=Nt[t-1,3]
m=0
k=0.0001
beta=1-exp(-k*I)
gamma=0.3
ps=1
pi=0.95
pr=1
TT = matrix(c((1-beta)*ps,beta*ps,0,0,(1-gamma)*pi,gamma*pi,0,0,ps),3,3)
FF = matrix(c(ps*m,0,0,pi*m,0,0,ps*m,0,0),3,3)
AA = TT+FF
Nt[t,]=round(AA %*% Nt[t-1,]) #added demographic stochasticity
}
# simulate data for jags
N=data.frame(S=rpois(nrow(Nt), Nt[,1]),
I=rpois(nrow(Nt), Nt[,2]),
R=rpois(nrow(Nt), Nt[,3]))
matplot(N,lty=1,type='l')
citation("rayshader")
# Load required packages
library(raster)
library(elevatr)
library(viridis)
# Load required packages
library(raster)
# Load required packages
library(raster)
# Load required packages
library(raster)
# Download the github .zip file
file <- tempfile()
download.file(
url      = "https://github.com/DavidDHofmann/MajorWaters/archive/master.zip"
, destfile = file
)
# Unzip the .zip file
unzip(zipfile = file)
# Load it
water <- shapefile(paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp"))
# Load it
water <- shapefile(paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp"))
# Visualize them
plot(swiss)
# Load required packages
library(raster)
library(elevatr)
library(viridis)
library(rayshader)
# Download country data (only on the national level -> level = 0)
swiss <- getData("GADM", country = "CHE", level = 0)
plot(swiss)
# Download DEM data
elev <- getData("alt", country = "CHE")
# Visualize it
plot(elev, col = viridis(50), box = F, axes = F, horizontal = T)
# Calculate resolution of raster (roughly in meters)
res(elev) * 111000
# Download DEM data
elev <- get_elev_raster(swiss, clip = "bbox", z = 7)
# Download DEM data
elev <- get_elev_raster(swiss, clip = "bbox", z = 7)
# Visualize it
plot(elev, col = viridis(50), box = F, axes = F, horizontal = T)
plot(swiss, border = "white", add = T)
# Calculate "aspect" and "slope"
aspect <- terrain(elev, "aspect")
slope <- terrain(elev, "slope")
# And now we can calculate and plot the hillshade
hillshade <- hillShade(slope = slope, aspect = aspect, angle = 45, direciton = 90)
plot(hillshade, col = gray(0:100/100), box = F, axes = F, legend = F)
plot(swiss, add = T)
plot(hillshade, col = gray(0:100/100), box = F, axes = F, legend = F)
plot(swiss, add = T)
# Define weights (size of moving window)
weights <- matrix(1, nrow = 3, ncol = 3)
# Function to calculate the terrain roughness index
roughness <- function(x){sum(abs(x[-5] - x[5])) / 8}
# Apply the function to the moving window
tri <- focal(elev, w = weights , fun = roughness, pad = TRUE, padValue = NA)
# Apply the function to the moving window
tri <- focal(elev, w = weights , fun = roughness, pad = TRUE, padValue = NA)
# Visualize
plot(tri, col = viridis(50), box = F, axes = F, horizontal = T)
# Convert the rasters to matrices
elmat <- raster_to_matrix(elev)
# Simple plot first
elmat %>%
sphere_shade(., texture = "imhof1") %>%
plot_map()
# Remove elevation outside switzerland
elev <- mask(elev, swiss)
# Convert the raster to a matrix
elmat <- raster_to_matrix(elev)
# Visualize
elmat %>%
sphere_shade(., texture = "imhof1") %>%
plot_map()
# Download the github .zip file
file <- tempfile()
download.file(
url      = "https://github.com/DavidDHofmann/MajorWaters/archive/master.zip"
, destfile = file
)
# Unzip the .zip file
unzip(zipfile = file)
# Load it
water <- shapefile(paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp"))
paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp")
# Load it
filename <- paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp")
# Load it
filename <- paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp")
water <- shapefile(filename)
file.exists(filename)
file
dirname(file)
dirname(file), "/MajorWaters-master/geo_che_water.shp"
paste0(dirname(file), "/MajorWaters-master/geo_che_water.shp")
# Unzip the .zip file
unzip(zipfile = file)
unzip(zipfile = file)
unzip(zipfile = file)
file
file
file
file
file
# Download the github .zip file
download.file("https://github.com/DavidDHofmann/MajorWaters/blob/master/geo_che_water.dbf")
# Load watermaps
water <- shapefile("Swiss_Waters.shp")
# Load watermaps
water <- shapefile("/home/david/ownCloud/Dokumente/Bibliothek/Wissen/R-Scripts/Swiss_Waters.shp")
# Load watermaps
water <- shapefile("/home/david/ownCloud/Dokumente/Bibliothek/Wissen/R-Scripts/Swiss_Waters2.shp")
# Load watermaps
water <- shapefile("/home/david/ownCloud/Dokumente/Bibliothek/Wissen/R-Scripts/Swiss_Water.shp")
# Load required packages
library(raster)     # To manipulate raster data
library(elevatr)    # To download elevation data
library(viridis)    # To get nice color palettes
library(rayshader)  # To plot stuff in 3D
# Download country data (only on the national level -> level = 0)
swiss <- getData("GADM", country = "CHE", level = 0)
dat <- readRDS("Downloads/dat.all")
dat <- readRDS("/home/david/Downloads/dat.all")
dat <- readRDS("/home/david/Downloads/dat.all.RData")
View(dat)
table(dat$dogName)
dat <- subset(dat, periodStart > "2015-10-01")
unique(dat$dogName)
################################################################################
#### Step Selection Function - Model Selection
################################################################################
# Description: In this script I run forward model selection using the 4-hourly
# fixes of our dispersers
# Clear R's brain
rm(list = ls())
# Surpress scientific notation
options(scipen = 999)
# Change the working directory
wd <- "/home/david/ownCloud/University/15. PhD/Chapter_1"
setwd(wd)
# Load required packages
library(tidyverse)    # For data wrangling
library(davidoff)     # Custom functions
library(parallel)     # To run stuff in parallel
library(corrplot)     # To plot correlations
library(cowplot)      # For nice plots
library(ggpubr)       # For nice plots
library(glmmTMB)      # For modelling
################################################################################
#### Comparison to Old Data
################################################################################
# I want to compare the current dataset to the dataset used in Hofmann et al
# 2020. Let's thus load the two
old <- read_csv("/home/david/ownCloud/University/15. PhD/Chapter_0/03_Data/02_CleanData/00_General_Dispersers_Popecol(SSF4Hours).csv")
new <- read_csv("/home/david/ownCloud/University/15. PhD/Chapter_1/03_Data/02_CleanData/00_General_Dispersers_POPECOL(iSSF_Extracted).csv")
# Subset to case_ steps only
old <- subset(old, case_)
new <- subset(new, case_ == 1)
# Remove undesired columns
old <- select(old, c(id, x1_, x2_, t1_, t2_))
new <- select(new, c(id, x1_, x2_, t1_, t2_))
# Let's only compare the data of the common individuals
new <- subset(new, id %in% old$id)
old <- subset(old, id %in% new$id)
# Create new column indicating dataset
old$set <- "old"
new$set <- "new"
# Combine them
comb <- rbind(old, new)
# Compare number of observations
table(comb$set)
diff(as.data.frame(table(comb$set))$Freq)
# Compare fixes per individual
table(comb$id, comb$set)
# Let's find all rows that are new
difference <- anti_join(new, old, by = c("id", "x1_", "x2_", "t1_", "t2_"))
joined <- full_join(old, new, by = c("id", "x1_", "x2_", "t1_", "t2_"))
View(joined)
################################################################################
#### Step Selection Function - Model Selection
################################################################################
# Description: In this script I run forward model selection using the 4-hourly
# fixes of our dispersers
# Clear R's brain
rm(list = ls())
# Surpress scientific notation
options(scipen = 999)
# Change the working directory
wd <- "/home/david/ownCloud/University/15. PhD/Chapter_1"
setwd(wd)
# Load required packages
library(tidyverse)    # For data wrangling
library(davidoff)     # Custom functions
library(parallel)     # To run stuff in parallel
library(corrplot)     # To plot correlations
library(cowplot)      # For nice plots
library(ggpubr)       # For nice plots
library(glmmTMB)      # For modelling
################################################################################
#### Comparison to Old Data
################################################################################
# I want to compare the current dataset to the dataset used in Hofmann et al
# 2020. Let's thus load the two
old <- read_csv("/home/david/ownCloud/University/15. PhD/Chapter_0/03_Data/02_CleanData/00_General_Dispersers_Popecol(SSF4Hours).csv")
new <- read_csv("/home/david/ownCloud/University/15. PhD/Chapter_1/03_Data/02_CleanData/00_General_Dispersers_POPECOL(iSSF_Extracted).csv")
# Subset to case_ steps only
old <- subset(old, case_)
new <- subset(new, case_ == 1)
# Remove undesired columns
old <- select(old, c(id, x1_, x2_, y1_, y2_, t1_, t2_))
new <- select(new, c(id, x1_, x2_, y1_, y2_, t1_, t2_))
# Let's only compare the data of the common individuals
new <- subset(new, id %in% old$id)
old <- subset(old, id %in% new$id)
# Create new column indicating dataset
old$set <- "old"
new$set <- "new"
# Combine them
comb <- rbind(old, new)
# Compare number of observations
table(comb$set)
diff(as.data.frame(table(comb$set))$Freq)
# Compare fixes per individual
table(comb$id, comb$set)
# Join the dataframes
joined <- full_join(old, new, by = c("id", "t1_", "t2_"))
# Sort the columns
joined <- select(joined, c(
id
, t1_
, t2_
, x1_old = x1_.x
, x1_new = x1_.y
, x2_old = x2_.x
, x2_new = x2_.y
, y1_old = y1_.x
, y1_new = y1_.y
, y2_old = y2_.x
, y2_new = y2_.y
, everything()
))
View(joined)
