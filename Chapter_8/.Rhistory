geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
test2
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line()
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1)
test2$sunrise
# Plot metrics
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunset, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunset_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
test2$sunrise_next
test2$sunset_next
# Plot metrics
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunset, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunrise_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
rm(list = ls())
# Load required packages
library(nimble)
n <- 1000
shape <- c(1, 2)
scale <- c(1, 4)
t <- matrix(c(0.9, 0.1, 0.1, 0.9), nrow = 2)
# Vectors to keep track of
s <- rep(NA, n) # States
x <- rep(NA, n) # Values
# Given the transition matrix, we can calculate the steady state state
# probabilities
d <- solve(t(diag(2) - t + 1), rep(1, 2))
# Simulate data
s[1] <- sample(c(1, 2), size = 1, prob = d)
x[1] <- rgamma(1, shape = shape[s[1]], scale = scale[s[1]])
for (i in 2:n) {
s[i] <- sample(1:2, size = 1, prob = t[s[i-1], ])
x[i] <- rgamma(1, shape = shape[s[i]], scale = scale[s[i]])
}
# Visualize
plot(x, pch = 20, cex = 1, col = c("orange", "cornflowerblue")[s])
################################################################################
#### Maximum Likelihood Estimation
################################################################################
# Store the true parameters as vector
theta_true <- c(t[1, 1], t[2, 2], shape, scale)
# Compute likelihood for a given theta
lik <- function(theta_star, x) {
# Backtransform theta
theta <- c(
plogis(theta_star[1])
, plogis(theta_star[2])
, exp(theta_star[3])
, exp(theta_star[4])
, exp(theta_star[5])
, exp(theta_star[6])
)
# Transition matrix and steady state
t <- matrix(c(theta[1], 1 - theta[2], 1 - theta[1], theta[2]), nrow = 2)
d <- solve(t(diag(2) - t + 1), c(1, 1))
# Shape and scale parameters for the two states
shape <- theta[3:4]
scale <- theta[5:6]
# Compute probabilities outside the loop -> Maybe this needs to be inside the
# loop for the bayesian approach
probs <- cbind(
dgamma(x, shape = shape[1], scale = scale[1])
, dgamma(x, shape = shape[2], scale = scale[2])
)
# Forward algorithm
foo <- d %*% diag(probs[1, ])
l <- log(sum(foo))       # to avoid numerical issues
phi <- foo / sum(foo)    # to avoid numerical issues
for (i in 2:length(x)) {
foo <- phi %*% t %*% diag(probs[i, ])
l <- l + log(sum(foo)) # to avoid numerical issues
phi <- foo / sum(foo)  # to avoid numerical issues
}
# We want to minimize the log-likelihood
return(-l)
}
# Try it!
theta_star <- c(
qlogis(0.7)
, qlogis(0.7)
, log(0.5)
, log(2.5)
, log(0.5)
, log(3)
)
ml <- nlm(lik, theta_star, x = x)
theta_est <- ml$estimate
theta_est <- c(
plogis(theta_est[1])
, plogis(theta_est[2])
, exp(theta_est[3])
, exp(theta_est[4])
, exp(theta_est[5])
, exp(theta_est[6])
)
# Compare to truth
cbind(theta_true, theta_est)
# nimble code to fit Gamma GLMM for dispersal duration:
hmm.code <- nimbleCode({
## PRIORS ##
# Prior for shape parameters
for(i in 1:Nstates){ # we loop over all states
shape[i] ~ dgamma(shape = 0.01, rate = 0.01) ## The closest you can get to a weakly informed prior is to use very small shape and rate parameters. (Then, the posterior depends very largely on the likelihood and hardly at all on the prior
#shape[i] ~ cauchy(0,2.5)  ## weakly informed prior per Gelman (2006)
}
# Prior for rate parameters
for(i in 1:Nstates){ # we loop over all states
rate[i] ~ dgamma(shape = 0.01, rate = 0.01) ## The closest you can get to a weakly informed prior is to use very small shape and rate parameters. (Then, the posterior depends very largely on the likelihood and hardly at all on the prior
#scale[i] ~ cauchy(0,2.5)  ## weakly informed prior per Gelman (2006)
}
# Prior for transition probabilities:
for(i in 1:Nstates){ # we loop over all possible transitions
t[i,1] ~ dunif(min = 0, max = 1)  ## uniform prior between 0 and 1
t[i,2] <- 1-t[i,1]
}
## LIKELIHOOD ##
# likelihood of the first observation
s[1] ~ dcat(prob = d[1:2])
x[1] ~ dgamma(shape = shape[s[1]], scale = 1/rate[s[1]])
# we loop over all remaining observations to get the joint likelihood of the full sequence:
for (i in 2:N) {
s[i] ~ dcat(prob = t[s[i-1],1:2])
x[i] ~ dgamma(shape = shape[s[i]], scale = 1/rate[s[i]])
}
scale[1] <- 1/rate[1]
scale[2] <- 1/rate[2]
})
# set up the model:
hmm.constants <- list(N = n, # number of observations
d = d,
Nstates = length(unique(s)))
hmm.data  <- list(x = x)
# we generate initial data
t.init <- matrix(c(runif(n = 1, min = 0, max = 1), runif(n = 1, min = 0, max = 1), NA, NA), nrow = 2)
t.init[,2] <- 1-t.init[,1]
hmm.inits <- list(shape = rgamma(n = hmm.constants$Nstates, shape = 0.01, rate = 0.01),
rate = rgamma(n = hmm.constants$Nstates, shape = 0.01, rate = 0.01),
t = t.init,
s = rcat(n = hmm.constants$N, prob = c(0.5, 0.5)))
hmm.model <- nimbleModel(code = hmm.code, constants = hmm.constants, inits = hmm.inits, data = hmm.data)
# fit the model:
hmm.fitted <- nimbleMCMC(model = hmm.model, thin = 1, nchains = 4, nburnin = 500, niter = 5000, samplesAsCodaMCMC = TRUE, monitors = c("shape","scale","t"))
# let's have a visual look at convergence:
mcmcplot(hmm.fitted)
library(mcmcplots)
library(runjags)
# let's have a visual look at convergence:
mcmcplot(hmm.fitted)
# get summary of model fit:
summary(hmm.fitted)
# combine the posterior values of all chains into a single dataframe:
df.hmm.fitted <- as.data.frame(combine.mcmc(mcmc.objects = hmm.fitted))
head(df.hmm.fitted)
# Extract a summary from a nimble model
nimble_summary <- function(samples, params = NULL, digits = 3) {
if (!is.null(params)){
samples <- jagsUI:::order.params(samples, params, FALSE, FALSE)
}
mat     <- as.matrix(samples)
nchain  <- length(samples)
niter   <- nrow(samples[[1]])
rhat    <- sapply(1:ncol(samples[[1]]), function(i) {
coda::gelman.diag(samples[,i], autoburnin = FALSE)$psrf[1, 1]
})
stats <- t(apply(mat, 2, function(x) {
x <- na.omit(x)
c(mean = mean(x), sd = sd(x), quantile(x, c(0.025, 0.5, 0.975)))
}))
out <- data.frame(stats, rhat = rhat, check.names = FALSE)
cat("Estimates based on", nchain, "chains of", niter, "iterations\n")
round(out, digits = digits)
}
nimble_summary(hmm.fitted)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `1` = rnorm(5)
)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `Patch1` = rnorm(5)
, `Patch2` = rnorm(5)
, `Patch3` = rnorm(5)
, `Patch4` = rnorm(5)
, `Patch5` = rnorm(5)
)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
)
sds <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5) / 10, 3)
, `Patch2` = round(rnorm(5) / 10, 3)
, `Patch3` = round(rnorm(5) / 10, 3)
, `Patch4` = round(rnorm(5) / 10, 3)
, `Patch5` = round(rnorm(5) / 10, 3)
)
# Create a table that looks like the IPC table
means <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
)
means
sds
# Create a table that looks like the IPC table
means <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
, Metric   = "Mean"
)
sds <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5) / 10, 3)
, `Patch2` = round(rnorm(5) / 10, 3)
, `Patch3` = round(rnorm(5) / 10, 3)
, `Patch4` = round(rnorm(5) / 10, 3)
, `Patch5` = round(rnorm(5) / 10, 3)
, Metric   = "SD"
)
rbind(means, sds)
both <- rbind(means, sds)
both <- dplyr::arrange(both, To, Metric)
both
both$Metric <- NULL
# Make a nice looking table
kbl(both)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
library(floodmaprs)
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
testi <- modis_load(downloaded)
downloaded
testi <- modis_load(downloaded[1])
modis_percentiles(testi)
modis_specs(testi)
modis_bimodal(testi)
plot(testi)
plot(modis_classify(testi))
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
url <- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'
flights <- read.csv(url)
flights$info <- paste0("<b>",flights$airport1, " - ", flights$airport2, "</b>")
install.packages("mapdeck")
View(flights)
View(flights)
library(mapdeck)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data = flights
, origin = c("start_lon", "start_lat")
, destination = c("end_lon", "end_lat")
, stroke_from = "airport1"
, stroke_to = "airport2"
, tooltip = "info"
, layer_id = 'arclayer'
)
library(mapdeck)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data = flights
, origin = c("start_lon", "start_lat")
, destination = c("end_lon", "end_lat")
, stroke_from = "airport1"
, stroke_to = "airport2"
, tooltip = "info"
, layer_id = 'arclayer'
)
url <- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'
flights <- read.csv(url)
flights$info <- paste0("<b>",flights$airport1, " - ", flights$airport2, "</b>")
library(mapdeck)
url <- 'https://raw.githubusercontent.com/plotly/datasets/master/2011_february_aa_flight_paths.csv'
flights <- read.csv(url)
flights$info <- paste0("<b>",flights$airport1, " - ", flights$airport2, "</b>")
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data = flights
, origin = c("start_lon", "start_lat")
, destination = c("end_lon", "end_lat")
, stroke_from = "airport1"
, stroke_to = "airport2"
, tooltip = "info"
, layer_id = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data = flights
, origin = c("start_lon", "start_lat")
, destination = c("end_lon", "end_lat")
, stroke_from = "airport1"
, stroke_to = "airport2"
, tooltip = "info"
, layer_id = 'arclayer'
)
library(tidyverse)
################################################################################
#### Table of Inter-Patch Connectivity
################################################################################
# Clear R's brain
rm(list = ls())
# Change the working directory
wd <- "/home/david/ownCloud/University/15. PhD/Chapter_8"
setwd(wd)
# Load required packages
library(tidyverse)    # To wrangle data
library(kableExtra)   # To generate a nice table
library(sf)           # To handle spatial data
library(ggspatial)    # For scalebar and north arrow
library(igraph)       # For network analysis
library(ggnetwork)    # To plot networks
library(terra)        # To handle raster files
library(RColorBrewer) # For custom colors
library(ggpubr)       # To arrange multiple plots
# Load results on interpatch connectivity
dat <- read_rds("03_Data/03_Results/BootstrappedInterpatchConnectivity.rds")
# Load source areas and the two watermaps
areas <- read_sf("03_Data/02_CleanData/SourceAreas.shp")
areas <- st_make_valid(areas)
water  <- rast("03_Data/02_CleanData/WaterCover.tif")[[c("min", "max")]]
water  <- crop(water, ext(21.5, 24.3, -20.7, -18))
water  <- as.data.frame(water, xy = T) %>% pivot_longer(3:4
, names_to  = "FloodLevel"
, values_to = "Water"
)
water$FloodLevel <- ifelse(water$FloodLevel == "min", "Min", "Max")
water$FloodLevel <- factor(water$FloodLevel, levels = c("Min", "Max"))
dat$FloodLevel   <- factor(dat$FloodLevel, levels = c("Min", "Max"))
# Prepare centroids
lay <- st_point_on_surface(areas)
lay <- st_coordinates(lay)
# Generate area labels
labels_areas <- st_coordinates(st_point_on_surface(areas))
labels_areas <- cbind(labels_areas, st_drop_geometry(areas))
sub <- subset(dat, FloodLevel == "Min")
sub <- select(dat, SourceArea, CurrentArea, Freq, StepNumber, FloodLevel)
net <- graph_from_data_frame(
d        = sub
, vertices = unique(areas$ID)
, directed = T
)
net_p <- suppressWarnings(ggnetwork(net, layout = lay, arrow.gap = 0.1, scale = F))
net_p <- subset(net_p, !is.na(FloodLevel))
library(mapdeck)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
net_p
sub <- subset(dat, FloodLevel == "Min")
sub <- select(sub, SourceArea, CurrentArea, Freq, StepNumber, FloodLevel)
net <- graph_from_data_frame(
d        = sub
, vertices = unique(areas$ID)
, directed = T
)
net_p <- suppressWarnings(ggnetwork(net, layout = lay, arrow.gap = 0.1, scale = F))
net_p <- subset(net_p, !is.na(FloodLevel))
library(mapdeck)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ", style = mapdeck_style('dark')) %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 20
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 10
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 5
, stroke_from_opacity = 0.5
, stroke_to_opacity = 0.5
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 5
, stroke_from_opacity = 0.9
, stroke_from_opacity = 0.9
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 5
, stroke_from_opacity = 0.9
, stroke_to_opacity = 0.9
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
mapdeck(token = "pk.eyJ1IjoiZG9keDkiLCJhIjoiY2p3dnltejJjMGR4YjN5bXp0ZjA2ZXBzMCJ9.4hirgQ-1SfJ2KHI7SR54cQ") %>%
add_arc(
data        = net_p
, origin      = c("x", "y")
, stroke_from = "Freq"
, stroke_to = "Freq"
, stroke_width = 5
, destination = c("xend", "yend")
, layer_id    = 'arclayer'
)
