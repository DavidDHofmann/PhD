pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunrise, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunset_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
################################################################################
#### Some Manual Verifications
################################################################################
# Let's see how the moon moves over Zurich
test1 <- calculateMoonlightIntensity(
date = seq(ymd_hms("2022-08-26 12:00:00"), ymd_hms("2022-08-27 12:00:00"), by = "15 mins")
, lat  = 47.373878
, lon  = 8.545094
, e    = 0.24
)
test2 <- moonlightSummary(
date = ymd("2022-08-26")
, lat  = 47.373878
, lon  = 8.545094
, e    = 0.24
, t    = "15 mins"
)
# Plot metrics
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunrise, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunset_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
test2
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line()
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1)
test2$sunrise
# Plot metrics
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunset, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunset_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
test2$sunrise_next
test2$sunset_next
# Plot metrics
test1 %>%
pivot_longer(sunAltDegrees:moonAltDegrees, names_to = "Variable", values_to = "Value") %>%
ggplot(aes(x = date, y = Value)) +
geom_line() +
facet_wrap(~ Variable, scale = "free", ncol = 1) +
geom_vline(xintercept = test2$sunset, lty = 2, col = "blue") +
geom_vline(xintercept = test2$sunrise_next, lty = 2, col = "blue") +
geom_vline(xintercept = test2$maxMoonTime, lty = 1, col = "blue") +
theme_minimal()
rm(list = ls())
# Load required packages
library(nimble)
n <- 1000
shape <- c(1, 2)
scale <- c(1, 4)
t <- matrix(c(0.9, 0.1, 0.1, 0.9), nrow = 2)
# Vectors to keep track of
s <- rep(NA, n) # States
x <- rep(NA, n) # Values
# Given the transition matrix, we can calculate the steady state state
# probabilities
d <- solve(t(diag(2) - t + 1), rep(1, 2))
# Simulate data
s[1] <- sample(c(1, 2), size = 1, prob = d)
x[1] <- rgamma(1, shape = shape[s[1]], scale = scale[s[1]])
for (i in 2:n) {
s[i] <- sample(1:2, size = 1, prob = t[s[i-1], ])
x[i] <- rgamma(1, shape = shape[s[i]], scale = scale[s[i]])
}
# Visualize
plot(x, pch = 20, cex = 1, col = c("orange", "cornflowerblue")[s])
################################################################################
#### Maximum Likelihood Estimation
################################################################################
# Store the true parameters as vector
theta_true <- c(t[1, 1], t[2, 2], shape, scale)
# Compute likelihood for a given theta
lik <- function(theta_star, x) {
# Backtransform theta
theta <- c(
plogis(theta_star[1])
, plogis(theta_star[2])
, exp(theta_star[3])
, exp(theta_star[4])
, exp(theta_star[5])
, exp(theta_star[6])
)
# Transition matrix and steady state
t <- matrix(c(theta[1], 1 - theta[2], 1 - theta[1], theta[2]), nrow = 2)
d <- solve(t(diag(2) - t + 1), c(1, 1))
# Shape and scale parameters for the two states
shape <- theta[3:4]
scale <- theta[5:6]
# Compute probabilities outside the loop -> Maybe this needs to be inside the
# loop for the bayesian approach
probs <- cbind(
dgamma(x, shape = shape[1], scale = scale[1])
, dgamma(x, shape = shape[2], scale = scale[2])
)
# Forward algorithm
foo <- d %*% diag(probs[1, ])
l <- log(sum(foo))       # to avoid numerical issues
phi <- foo / sum(foo)    # to avoid numerical issues
for (i in 2:length(x)) {
foo <- phi %*% t %*% diag(probs[i, ])
l <- l + log(sum(foo)) # to avoid numerical issues
phi <- foo / sum(foo)  # to avoid numerical issues
}
# We want to minimize the log-likelihood
return(-l)
}
# Try it!
theta_star <- c(
qlogis(0.7)
, qlogis(0.7)
, log(0.5)
, log(2.5)
, log(0.5)
, log(3)
)
ml <- nlm(lik, theta_star, x = x)
theta_est <- ml$estimate
theta_est <- c(
plogis(theta_est[1])
, plogis(theta_est[2])
, exp(theta_est[3])
, exp(theta_est[4])
, exp(theta_est[5])
, exp(theta_est[6])
)
# Compare to truth
cbind(theta_true, theta_est)
# nimble code to fit Gamma GLMM for dispersal duration:
hmm.code <- nimbleCode({
## PRIORS ##
# Prior for shape parameters
for(i in 1:Nstates){ # we loop over all states
shape[i] ~ dgamma(shape = 0.01, rate = 0.01) ## The closest you can get to a weakly informed prior is to use very small shape and rate parameters. (Then, the posterior depends very largely on the likelihood and hardly at all on the prior
#shape[i] ~ cauchy(0,2.5)  ## weakly informed prior per Gelman (2006)
}
# Prior for rate parameters
for(i in 1:Nstates){ # we loop over all states
rate[i] ~ dgamma(shape = 0.01, rate = 0.01) ## The closest you can get to a weakly informed prior is to use very small shape and rate parameters. (Then, the posterior depends very largely on the likelihood and hardly at all on the prior
#scale[i] ~ cauchy(0,2.5)  ## weakly informed prior per Gelman (2006)
}
# Prior for transition probabilities:
for(i in 1:Nstates){ # we loop over all possible transitions
t[i,1] ~ dunif(min = 0, max = 1)  ## uniform prior between 0 and 1
t[i,2] <- 1-t[i,1]
}
## LIKELIHOOD ##
# likelihood of the first observation
s[1] ~ dcat(prob = d[1:2])
x[1] ~ dgamma(shape = shape[s[1]], scale = 1/rate[s[1]])
# we loop over all remaining observations to get the joint likelihood of the full sequence:
for (i in 2:N) {
s[i] ~ dcat(prob = t[s[i-1],1:2])
x[i] ~ dgamma(shape = shape[s[i]], scale = 1/rate[s[i]])
}
scale[1] <- 1/rate[1]
scale[2] <- 1/rate[2]
})
# set up the model:
hmm.constants <- list(N = n, # number of observations
d = d,
Nstates = length(unique(s)))
hmm.data  <- list(x = x)
# we generate initial data
t.init <- matrix(c(runif(n = 1, min = 0, max = 1), runif(n = 1, min = 0, max = 1), NA, NA), nrow = 2)
t.init[,2] <- 1-t.init[,1]
hmm.inits <- list(shape = rgamma(n = hmm.constants$Nstates, shape = 0.01, rate = 0.01),
rate = rgamma(n = hmm.constants$Nstates, shape = 0.01, rate = 0.01),
t = t.init,
s = rcat(n = hmm.constants$N, prob = c(0.5, 0.5)))
hmm.model <- nimbleModel(code = hmm.code, constants = hmm.constants, inits = hmm.inits, data = hmm.data)
# fit the model:
hmm.fitted <- nimbleMCMC(model = hmm.model, thin = 1, nchains = 4, nburnin = 500, niter = 5000, samplesAsCodaMCMC = TRUE, monitors = c("shape","scale","t"))
# let's have a visual look at convergence:
mcmcplot(hmm.fitted)
library(mcmcplots)
library(runjags)
# let's have a visual look at convergence:
mcmcplot(hmm.fitted)
# get summary of model fit:
summary(hmm.fitted)
# combine the posterior values of all chains into a single dataframe:
df.hmm.fitted <- as.data.frame(combine.mcmc(mcmc.objects = hmm.fitted))
head(df.hmm.fitted)
# Extract a summary from a nimble model
nimble_summary <- function(samples, params = NULL, digits = 3) {
if (!is.null(params)){
samples <- jagsUI:::order.params(samples, params, FALSE, FALSE)
}
mat     <- as.matrix(samples)
nchain  <- length(samples)
niter   <- nrow(samples[[1]])
rhat    <- sapply(1:ncol(samples[[1]]), function(i) {
coda::gelman.diag(samples[,i], autoburnin = FALSE)$psrf[1, 1]
})
stats <- t(apply(mat, 2, function(x) {
x <- na.omit(x)
c(mean = mean(x), sd = sd(x), quantile(x, c(0.025, 0.5, 0.975)))
}))
out <- data.frame(stats, rhat = rhat, check.names = FALSE)
cat("Estimates based on", nchain, "chains of", niter, "iterations\n")
round(out, digits = digits)
}
nimble_summary(hmm.fitted)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `1` = rnorm(5)
)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `Patch1` = rnorm(5)
, `Patch2` = rnorm(5)
, `Patch3` = rnorm(5)
, `Patch4` = rnorm(5)
, `Patch5` = rnorm(5)
)
# Create a table that looks like the IPC table
data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
)
sds <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5) / 10, 3)
, `Patch2` = round(rnorm(5) / 10, 3)
, `Patch3` = round(rnorm(5) / 10, 3)
, `Patch4` = round(rnorm(5) / 10, 3)
, `Patch5` = round(rnorm(5) / 10, 3)
)
# Create a table that looks like the IPC table
means <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
)
means
sds
# Create a table that looks like the IPC table
means <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5), 2)
, `Patch2` = round(rnorm(5), 2)
, `Patch3` = round(rnorm(5), 2)
, `Patch4` = round(rnorm(5), 2)
, `Patch5` = round(rnorm(5), 2)
, Metric   = "Mean"
)
sds <- data.frame(
To  = c(1:5)
, `Patch1` = round(rnorm(5) / 10, 3)
, `Patch2` = round(rnorm(5) / 10, 3)
, `Patch3` = round(rnorm(5) / 10, 3)
, `Patch4` = round(rnorm(5) / 10, 3)
, `Patch5` = round(rnorm(5) / 10, 3)
, Metric   = "SD"
)
rbind(means, sds)
both <- rbind(means, sds)
both <- dplyr::arrange(both, To, Metric)
both
both$Metric <- NULL
# Make a nice looking table
kbl(both)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
library(floodmaprs)
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
testi <- modis_load(downloaded)
downloaded
testi <- modis_load(downloaded[1])
modis_percentiles(testi)
modis_specs(testi)
modis_bimodal(testi)
plot(testi)
plot(modis_classify(testi))
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
library(floodmapr)
downloaded <- modis_download(
dates     = c("2017-08-21", "2017-08-22")
, outdir    = tempdir()
, tmpdir    = tempdir()
, username  = "DoDx9"
, password  = "EarthData99"
, overwrite = F
)
# Clear R's brain
rm(list = ls())
# Set working directory
wd <- "/home/david/ownCloud/University/15. PhD/Chapter_7"
setwd(wd)
# Load required packages
library(tidyverse)   # For data wrangling
library(lubridate)   # To handle dates
library(hms)         # To handle times
library(broom)       # To clean model outputs
# Load custom functions
source("02_R-Scripts/00_Functions.R")
# Load cleaned activity data (note that the timestamps are all in UTC)
dat <- read_csv("03_Data/02_CleanData/ActivityDataCovariatesAggregated.csv")
View(dat)
dat$NFixes      <- NULL
# For now, ignore dispersers
dat <- subset(dat, State == "Resident")
# Normalize covariate values to a range between 0 and 1 (I won't standardize
# them because they are not normally distributed)
dat <- mutate(dat
, minMoonlightIntensity  = normalize(minMoonlightIntensity)
, meanMoonlightIntensity = normalize(meanMoonlightIntensity)
, maxMoonlightIntensity  = normalize(maxMoonlightIntensity)
, minTemperature         = normalize(minTemperature)
, meanTemperature        = normalize(meanTemperature)
, maxTemperature         = normalize(maxTemperature)
, minPrecipitation       = normalize(minPrecipitation)
, meanPrecipitation      = normalize(meanPrecipitation)
, maxPrecipitation       = normalize(maxPrecipitation)
, meanCloudCover         = normalize(meanCloudCover)
, meanActX6              = normalize(meanActX6)
, meanActX12             = normalize(meanActX12)
, meanActX18             = normalize(meanActX18)
, meanActX24             = normalize(meanActX24)
, meanActX24             = normalize(meanCloudCoverNight)
, maxMoonDelay           = normalize(maxMoonDelay)
)
# Nest data by individual and tod
dat_nested <- dat %>% nest(Data = -c(DogID, ToD))
# Only work with rows for which there are at least 50 datapoints
dat_nested$N <- sapply(dat_nested$Data, function(x) {nrow(x)})
dat_nested <- subset(dat_nested, N >= 50)
################################################################################
#### Exploratory
################################################################################
# How does activity depend on moonlight intensity?
ggplot(dat, aes(x = maxMoonlightIntensity, y = meanActX)) +
geom_point() +
facet_wrap(~ ToD) +
theme_minimal() +
geom_smooth(method = "lm", formula = y ~ poly(x, 2))
# How does activity depend on precipitation?
ggplot(dat, aes(x = maxPrecipitation, y = meanActX)) +
geom_point() +
facet_wrap(~ ToD) +
theme_minimal() +
geom_smooth(method = "lm", formula = y ~ poly(x, 2))
# How does activity depend on rain (binary)?
ggplot(dat, aes(x = as.factor(Rain), y = meanActX)) +
geom_boxplot() +
facet_wrap(~ ToD) +
theme_minimal()
# Is there an interaction between the two?
ggplot(dat, aes(x = maxPrecipitation, y = meanActX, col = as.factor(Rain))) +
geom_point() +
facet_wrap(~ ToD) +
theme_minimal() +
geom_smooth(method = "lm", formula = y ~ poly(x, 2))
################################################################################
#### Analysis I - Average Activity by Time of the Day
################################################################################
# Decide on a model formula
formula <- quote(meanActX ~
+ maxMoonlightIntensity
+ I(maxMoonlightIntensity ** 2)
# + Rain
# + maxPrecipitation
# + Rain:maxPrecipitation
+ maxTemperature
# + I(maxTemperature ** 2)
# + meanCloudCover
+ meanCloudCoverNight
+ maxMoonlightIntensity:meanCloudCoverNight
+ maxMoonlightIntensity:maxMoonDelay
+ meanActX6
# + I(maxPrecipitation ** 2)
# + I(meanTemperature ** 2)
)
# Run a linear model for each dog separately for the different times of the day
dat_nested$Model <- lapply(dat_nested$Data, function(x) {
lm(formula, x)
})
# Extract model results
dat_nested$ModelCoefficients <- lapply(dat_nested$Model, function(x) {
broom::tidy(x)
})
# Unnest
coefs <- dat_nested %>%
select(DogID, ToD, ModelCoefficients) %>%
unnest(ModelCoefficients)
# Compute weight for each coefficient
coefs_means_weighted <- coefs %>%
nest(Coefs = -c(ToD, term)) %>%
mutate(Coefs = map(Coefs, function(x) {
be <- x$estimate
se <- x$std.error
we <- (1 / (se ** 2)) / sum(1 / (se ** 2))
be_mean <- sum(we * be)
se_mean <- sqrt(sum(we * (be - be_mean) ** 2) / (length(be) - 1))
result <- tibble(
estimate  = be_mean
, std.error = se_mean
, LWR       = estimate - 1.96 * std.error
, UPR       = estimate + 1.96 * std.error
)
return(result)
})) %>% unnest(Coefs)
# Compute mean and se of the model estimates (unweighted)
coefs_means_unweighted <- coefs %>%
group_by(ToD, term) %>%
summarize(
estimate_mean = mean(estimate)
, estimate_sd   = sd(estimate) / sqrt(n())
, LWR           = estimate_mean - qt(1 - 0.05 / 2, n() - 1) * estimate_sd  # UNWEIGHTED: TAKE A LOOK AT MURTAUGH AGAIN
, UPR           = estimate_mean + qt(1 - 0.05 / 2, n() - 1) * estimate_sd  # UNWEIGHTED: TAKE A LOOK AT MURTAUGH AGAIN
, .groups       = "drop"
)
# Visualize
coefs %>%
ggplot(aes(x = ToD, y = estimate, col = DogID)) +
geom_hline(yintercept = 0, lty = 2) +
geom_jitter(width = 0.1, alpha = 0.8) +
geom_point(
data        = coefs_means_unweighted
, mapping     = aes(x = ToD, y = estimate_mean)
, inherit.aes = F
, size        = 2
) +
geom_errorbar(
data        = coefs_means_unweighted
, mapping     = aes(x = ToD, ymin = LWR, ymax = UPR)
, inherit.aes = F
, width       = 0.2
, lwd         = 1
) +
# geom_violin(col = "black") + # Could also use a violin plot
facet_wrap(~ term, scales = "free") +
theme_minimal() +
scale_color_viridis_d() +
theme(legend.position = "none")
################################################################################
#### Analysis II - Time when Becoming Moving
################################################################################
# For this, we'll only work with "evening" data
evening <- subset(dat, ToD == "Evening")
# Compute time of the days as regular seconds since midnight
evening$StartMovingNumeric <- as.numeric(as_hms(evening$StartMoving))
# Some exploratory plots
ggplot(evening, aes(x = maxMoonlightIntensity, y = StartMovingNumeric)) +
geom_point() +
theme_minimal() +
geom_smooth(method = "lm", formula = y ~ poly(x, 2))
# How does the time of becoming active change throghout the year?
ggplot(evening, aes(x = yday(Date), y = as_hms(evening$StartMoving))) +
geom_point() +
geom_smooth(method = "lm", formula = y ~ poly(x, 2))
# Some exploratory plots
ggplot(evening, aes(x = maxMoonlightIntensity, y = as.numeric(as_hms(StartMoving)))) +
geom_point(aes(col = maxMoonDelay)) +
theme_minimal() +
geom_smooth(method = "lm") +
scale_color_viridis_c(option = "magma")
